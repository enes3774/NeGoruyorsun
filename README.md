# Ne GÃ¶rÃ¼yorsun?
Ne GÃ¶rÃ¼yorsun, Ã§eÅŸitli resimleri metinsel olarak tasvir eden GPT2 ve CLIP modellerini kullanan TÃ¼rkÃ§e DoÄŸal Dil Ä°ÅŸleme projesi.

Proje TakÄ±m AdÄ±: TEXT-GEN

![image](https://user-images.githubusercontent.com/77508537/184640351-05e43f6d-ea4b-459b-a4be-be21da65e717.png)

[![Kaggle](http://img.shields.io/static/v1?logo=kaggle&style=plastic&color=blue&label=kaggle&labelColor=grey&message=notebooks)](https://www.kaggle.com/code/eneskulak/ne-goruyorsun)
# Projenin Motivasyonu
GÃ¶rme engellilerin en bÃ¼yÃ¼k problemi dÄ±ÅŸ Ã§evreyi tanÄ±mlamaktÄ±r. Bu sorunu Ã§Ã¶zebilmek iÃ§in onlara dÄ±ÅŸ Ã§evreyi tasvir eden bir yapay zeka projesi geliÅŸtirdim.

Bu proje bir resim alÄ±r ve bu resmi analiz edip size yazÄ±yla tasvir eder. Model eÄŸitilirken kendi oluÅŸturduÄŸum wikipedia resim-metin veriseti, Turkish MSCOCO ve Tasvir Et verisetleri kullanÄ±lmÄ±ÅŸtÄ±r. Datasets kÄ±smÄ±ndan daha fazla ayrÄ±ntÄ±ya bakabilirsiniz.

Model Kaggle ortamÄ±nda yaklaÅŸÄ±k 10 saat eÄŸitildi. Her bir epoch 3 saat sÃ¼rdÃ¼ yani yaklaÅŸÄ±k 3 epoch eÄŸitilidi.

Teknofest Sunum Linki:https://docs.google.com/presentation/d/1U-gMP8bJkzaW_pu14nTu5CHa3S7SChhUdHkBMDg0tWQ
# Ã–rnek
https://user-images.githubusercontent.com/77508537/185377116-aaa2690e-6353-4684-9a59-dfea08501a05.mp4



# Modelin DetaylarÄ±
Proje iki kÄ±sÄ±mdan oluÅŸuyor, image encoder ve text decoder. Ä°mage encoder, verilen resmi 512 uzunluÄŸunda bir vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. 
Bu vektÃ¶r iÃ§inde resimdeki Ã¶nemli detaylarÄ±(eÅŸyalar,onlarÄ±n renkleri vs.) barÄ±ndÄ±rÄ±r. OluÅŸturulan bu vektÃ¶r baÅŸka kÃ¼Ã§Ã¼k bir model sayesinde text decoder'a verilmek iÃ§in boyutu arttÄ±rÄ±lÄ±r.
ArttÄ±rÄ±lan bu vektÃ¶r text decoder modeline verilir. Bu model ise aldÄ±ÄŸÄ± vektÃ¶re bakarak yeni bir metin oluÅŸturur. OluÅŸturulan bu metin, modelin resimde gÃ¶rdÃ¼kleri olur.

| ![CLIP](../main/images/model_sema.jpg) |
|:--:|
| Ä°mage Captioning Modeli |

Projeyi yaparken daha Ã¶nceden TÃ¼rkÃ§e verilerle eÄŸitilmiÅŸ  GPT-2 modeli kullanÄ±ldÄ±. Bunun nedeni modelin daha hÄ±zlÄ± TÃ¼rkÃ§e metinlere uyum saÄŸlamasÄ±ydÄ±. Ä°mage encoder olarak da OpenAI'Ä±n yayÄ±nlamÄ±ÅŸ olduÄŸu CLIP Image Encoder modeli kullanÄ±ldÄ±.
Bu modeli kullanmÄ±ÅŸ olmamÄ±n nedeni, modelin daha Ã¶nceden 400 milyon fotoÄŸraf Ã¼zerinde eÄŸitilmesi. Bu sayede model daha hÄ±zlÄ± optimize edilebildi.

# Model SonuÃ§larÄ±
 <table>
  <tr>
    <td><img src="../main/images/test1.jpg" width="300"></td>
    <td><img src="../main/images/test3.jpg" width="300"></td>
    <td><img src="../main/images/test5.png" width="300"></td>
  </tr>
  <tr>
    <td>Bir kÄ±z doÄŸum gÃ¼nÃ¼ pastasÄ±nda mumlarÄ± Ã¼fler.</td>
     <td> SÃ¶rf yapan bir adam.</td>
    <td>Bir grup otobÃ¼s, bir otoparkta park edildi.</td>
  </tr>
 </table>
 <table>
  <tr>
    <td><img src="../main/images/test2.png" width="300"></td>
    <td><img src="../main/images/test4.jpg" width="300"></td>
    <td><img src="../main/images/test6.jpg" width="300"></td>
  </tr>
  <tr>
    <td>Bir kÃ¶pek bir yÃ¶ne bakÄ±yor.</td>
     <td> ElmanÄ±n yakÄ±n plan gÃ¶rÃ¼ntÃ¼sÃ¼.</td>
    <td>Ã‡imlerin altÄ±nda duran bir yavru kedi.</td>
  </tr>
 </table>
 
 # Modeli EÄŸitmek 
 ### 1. Projeyi Kendi BilgisayarÄ±nÄ±za YÃ¼kleyin
  ```
  git clone enes3774/NeGoruyorsun 
  ```
 
 ### 2. BaÄŸÄ±mlÄ±lÄ±klarÄ± Ä°ndir
 Modeli eÄŸitmek iÃ§in Ã¶ncelikle requirements.txt dosyasÄ±ndaki kÃ¼tÃ¼phanelerin kurulmuÅŸ olmasÄ± gerekir.
  ```
  pip install -r requirements.txt
  ```
  
  Bir de CLIP kÃ¼tÃ¼phanesini github sayfasÄ±ndan yÃ¼klemeniz gerekiyor.
  ```
  pip install git+https://github.com/openai/CLIP.git
  ```
 
 ### 3. Ã–rnek Dataseti veya Kendi Datasetinizi Ä°ndirin
Modeli eÄŸitmek iÃ§in resimlere ve resimlerin TÃ¼rkÃ§e metinleri olan bir json dosyasÄ±na ihtiyacÄ±nÄ±z var. 
- Ã–rnek olarak MSCOCO resimlerini [buradan](https://www.kaggle.com/datasets/aftaab/mscoco) indirin ve proje dosyasÄ±nda "images_data/" klasorÃ¼ oluÅŸturup iÃ§ine atÄ±n. 
- Bu resimlerin TÃ¼rkÃ§e metinlerini barÄ±ndÄ±ran json dosyasÄ±nÄ± da [buradan](https://github.com/giddyyupp/turkish-image-captioning/blob/master/MSCOCO/train/coco_train_captions_tr.json) indirip "dataset.json" olarak proje dosyasÄ±na koyun. FaklÄ± verisetleri hakkÄ±nda daha fazla bilgiyi datasets klasorunde bulabilirsiniz.

Dataseti hazÄ±rladÄ±ktan dosya klasoru bu ÅŸekilde olmalÄ±: 

    .
    â”œâ”€â”€ images                  # Github iÃ§in kullanÄ±lan resimler burada 
    â”œâ”€â”€ datasets                # verisetleri ve kullanÄ±mÄ± hakkÄ±nda 
    â”œâ”€â”€ Ä°mageCaptioning         # Projenin modeli bu dosyada bulunuyor
    â”œâ”€â”€ images_data                  #iÃ§erisinde kullandÄ±ÄŸÄ±n verisetinin fotoÄŸraflarÄ± olmalÄ±
    â”œâ”€â”€ .gitignore            
    â”œâ”€â”€ dataset.json             #images iÃ§indeki resimlerin dosya adlarÄ± ve o resme karÅŸÄ±lÄ±k gelen metinleri barÄ±ndÄ±rÄ±yor 
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ README.md
 KullandÄ±ÄŸÄ±nÄ±z dataset flickr ise train.py kodunda 4. satÄ±rÄ± okuyun. 
 
  ### 4. Modeli EÄŸitmek
 
 Modeli eÄŸitmek iÃ§in  aÅŸaÄŸÄ±daki kodu yazÄ±n.
 ```
  python Ä°mageCaptioning/train.py
 ```
 Train dosyasÄ±ndaki hiperparametreleri(epoch sayÄ±sÄ±, batch_size, learning rate vb.) dÃ¼zenleyip modeldeki sonuÃ§larÄ± gÃ¶zlemleyebilirsiniz(ÅŸuanki hiperparametrelerde model doÄŸru sonuÃ§ verecektir.). Modeli bu veriyle 1 epoch eÄŸitmeniz bile yeterli olacaktÄ±r.
 
 # Test AÅŸamasÄ±
 ### 1. EÄŸitilen Modeli Kaydetmek
 EÄŸittiÄŸiniz modeli ImageCaptioning dosyasÄ± iÃ§inde "checkpoints/" klasorune "model_latest.pth" ÅŸeklinde koymanÄ±z gerekiyor(Modeli eÄŸitirken her 6000 adÄ±mda bir ve epoch sonunda kaydediliyor. Train ederken model doÄŸru yere kaydedilecektir.). HazÄ±r bir model checkpoint paylaÅŸmayacaÄŸÄ±m Ã§Ã¼nkÃ¼ model Ã§ok hÄ±zlÄ± bir ÅŸekilde(yaklaÅŸÄ±k 1-2 saate Ã§alÄ±ÅŸan bir model elde edebilirsiniz.) eÄŸitebiliyor olmanÄ±z.
 ### 2. Modeli Test Etmek
 Modeli belirtlien dosyaya koyduktan sonra test kodunu Ã§alÄ±ÅŸtÄ±rÄ±n. 
  ```
  python Ä°mageCaptioning/test.py
 ```
Test kodundaki resmi deÄŸiÅŸtirip farklÄ± resimler iÃ§in modeli test edebilirisiniz.
## ğŸ““ Kaggle

* Modeli [Kaggle](https://www.kaggle.com/code/eneskulak/ne-goruyorsun) ortamÄ±nda yaklaÅŸÄ±k 2 saatte eÄŸitebilirsiniz. Ä°Ã§erisinde eÄŸittiÄŸiniz modeli kullanmak iÃ§in beam_search ve greedy olmak Ã¼zere 2 algoritma bulunuyor.

## VideolarÄ± Tasvir Etmek

Bu model ile videolarÄ± tasvir etmek de mÃ¼mkÃ¼n. Bununla alakalÄ±   ```PredictVideoCaption.py``` kodunu paylaÅŸtÄ±m. Bunu yaparken verdiÄŸiniz videodan 5 saniyede bir resim alÄ±nÄ±p modelden geÃ§iriliyor. Elde ettiÄŸiniz metinler toplanÄ±yor ve bu sizin videoda ne yaptÄ±ÄŸÄ±nÄ±zÄ±n Ã¶zeti oluyor. Burada cÃ¼mleler birbirine benzeyebilir. Bunu Ã¶nlemek iÃ§in birbirine Ã§ok benzeyen cÃ¼mleleri atabilirsiniz.

Bunu kullanarak tam zamanlÄ± gÃ¶rÃ¼ntÃ¼ tasviri yapabilirsiniz.
### Ã–rnek
!!!LÃœTFEN SESÄ° AÃ‡IN






https://user-images.githubusercontent.com/77508537/185651778-a2a6df5e-5e48-403b-9446-7c94b232a52d.mp4






NOT: OluÅŸturulan cÃ¼mleler tamamen eÄŸittiÄŸim yapay zeka tarafÄ±ndan elde edildi. Projenin iÅŸlevselliÄŸini ortaya Ã§Ä±karmak iÃ§in bir video dÃ¼zenleyicisinden seslendirme ve altyazÄ± eklenmiÅŸtir. Bunu Google Text to Speech gibi bir API kullanarak otomatik ÅŸekilde yapmak mÃ¼mkÃ¼n.

## Risk PuanÄ± HesaplanmasÄ±
Elde edilen cÃ¼mleler Ã¼zerinde kullanÄ±cÄ±yÄ± tehdit eden durumlar tespit edilmesi ve kullanÄ±cÄ±yÄ± uyarmak iÃ§in risk puanÄ± hesaplanmÄ±ÅŸtÄ±r. Bununla alakalÄ± ```CalculateTestScore.ipynb``` kodu hazÄ±rlandÄ±. YaptÄ±ÄŸÄ± ÅŸey, verdiÄŸiniz metni almak ve bu cÃ¼mlenin bir gÃ¶rme engelli iÃ§in ne kadar tehlikeli olduÄŸunu dÃ¶ndÃ¼rmek.
Bunu yaparkenki aÅŸamalarÄ±:
1. CÃ¼mleler kelimelere ayrÄ±ldÄ± ve her bir kelimenin kÃ¶kÃ¼ alÄ±ndÄ±
2. AlÄ±nan kelime kÃ¶klerinin, daha Ã¶nceden verilen tehlikeli kelimelerle olan benzerliÄŸi hesaplandÄ±. Bu sayede o kelimenin ne kadar tehlikeli olduÄŸu Ã¶ÄŸrenildi.
3. Elde edilen kelimelerin tehlike puanlarÄ± toplanÄ±p o cÃ¼mlenin tehlike skoru hesaplandÄ±.

## Gelecek Hedefler
1. Veriyi GeniÅŸletmek
Modele gÃ¼nlÃ¼k hayatta olmayan veriler(animasyonlar, hayali figÃ¼rler, sanatlar) vererek modelin her konuda fikir sahibi olmasÄ±nÄ± saÄŸlamak.

2. Modelin videolarÄ± daha iyi anlamasÄ± iÃ§in frame deÄŸil birden Ã§ok frameâ€™i aynÄ± anda alÄ±p deÄŸerlendiren bir video encoder model kullanÄ±lmasÄ±.

3. Projenin mobil app halinde kullanÄ±labilir olmasÄ±.

4. Giyilebilir bir teknoloji ile gÃ¶rme engelliler iÃ§in projeyi hayata geÃ§irmek. EÄŸer model sonuÃ§larÄ± arttÄ±rÄ±l
